| No. | Question | Ans |
|---|---|---|
| 1 | **Deep-Reinforcement Learning >> Recurrent Neural Network** QNo. 1: What is the Exploding Gradient Problem in Recurrent Neural Network (RNNs)? A. It occurs when gradients become excessively large during backpropagation, causing unstable updates and making the training process difficult to converge. B. It happens when gradients vanish and become too small, preventing the network from learning long-term dependencies in sequences. C. It refers to the sudden increase in model size due to adding more hidden layers in an RNN architecture. D. It describes the excessive memory usage during training when sequences are too long for the network to process efficiently. | A |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (A): Exploding gradients occur when gradient values become excessively large during backpropagation, leading to unstable weight updates and hindering model training. $\times$ Explanation of Distractors: B: Incorrect — This describes the vanishing gradient problem, which is the opposite issue. C: Incorrect — This is unrelated to gradient problems; it concerns model complexity and memory use. D: Incorrect — Memory usage is a computational concern, not directly related to gradients. | |
| 2 | **Deep-Reinforcement Learning >> Recurrent Neural Network** QNo. 2: Which variables are most difficult to train in a recurrent neural network (RNN) example? A. Input-to-hidden weights, because they directly affect how the input features are initially processed by the network. B. Hidden-to-output weights, as they determine how the hidden state transforms into the final output prediction. C. Bias terms, since they shift the activation functions and require precise tuning for each neuron in the network. D. Hidden-to-hidden weights, because they are reused across time steps and can cause vanishing or exploding gradient problems during training. | D |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (D): Hidden-to-hidden weights are hardest to train as they propagate gradients over many time steps, often leading to vanishing or exploding gradients in RNNs. $\times$ Explanation of Distractors: A: Incorrect — Input to hidden weights are important but generally easier to train as they affect only current inputs. B: Incorrect — Hidden-to-output weights are trained using direct error signals, making their training more stable. C: Incorrect — Bias terms have fewer parameters and are less challenging compared to the recurrent weights spanning time steps. | |
| 3 | **Deep-Reinforcement Learning >> Recurrent Neural Network** QNo. 3: How is a Recurrent Neural Network (RNN) unfolded across time steps during training and inference? A. By replicating the same weights across different layers and stacking them vertically for parallel sequence processing at each time step. B. By duplicating the RNN cell across time steps to represent sequential processing, using the same weights at each time step. C. By reversing the input sequence and applying separate RNNs to each time step using different weights for each layer. D. By collapsing all time steps into one layer to compute all outputs simultaneously using completely independent parameter sets. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): Unfolding an RNN involves duplicating the RNN cell across each time step, maintaining shared weights to capture temporal dependencies in sequential data. $\times$ Explanation of Distractors: A: Incorrect — RNNs use shared weights across time, not vertically stacked layers for parallel processing. C: Incorrect — Reversing input is used in bi-directional RNNs, but each time step still shares the same weights. D: Incorrect — Collapsing time steps ignores sequential structure, and independent parameters break the temporal learning mechanism of RNNs. | |
| 4 | **Deep-Reinforcement Learning >> Recurrent Neural Network** QNo. 4: What are the different input-output architectures used in Recurrent Neural Networks (RNNs)? A. One-to-one and many-to-one, where the RNN maps all input sequences to a single fixed numerical output value. B. One-to-one, one-to-many, many-to-one, and many-to-many, depending on the nature of input and output sequence lengths. C. One-to-many and many-to-one only, because RNNs cannot process multiple inputs or outputs simultaneously in complex tasks. D. Many-to-none and many-to-many only, as RNNs are designed to ignore initial input steps for faster convergence. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): RNNs support one-to-one, one-to-many, many-to-one, and many-to-many architectures to flexibly handle various sequence modeling problems across time steps. $\times$ Explanation of Distractors: A: Incorrect — This describes some architectures, but not all four major types. C: Incorrect — RNNs can handle multiple inputs and outputs simultaneously (many-to-many). D: Incorrect — RNNs are designed to process the entire sequence context, not ignore initial steps. | |
| 5 | **Deep-Reinforcement Learning >> Recurrent Neural Network** QNo. 5: What are Recurrent Neural Networks (RNNs) and what type of problems are they designed to solve? A. RNNs are tree-based models that split data into branches and are used mainly for classification of static tabular data. B. RNNs are neural networks designed to process sequential data by maintaining hidden states that capture information from previous time steps. C. RNNs are convolution-based models used to extract spatial features from images for tasks like object detection and classification. D. RNNs are unsupervised clustering algorithms used to group similar data points based on distance or density thresholds. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): Recurrent Neural Networks handle sequential data by maintaining memory of past inputs, making them ideal for tasks like language modeling, time series, and speech recognition. $\times$ Explanation of Distractors: A: Incorrect — This describes decision trees, not RNNs; RNNs are not used for hierarchical splitting of static data. C: Incorrect — CNNs, not RNNs, specialize in spatial feature extraction for visual data like images. D: Incorrect — Clustering algorithms like K-Means or DBSCAN match this description, not neural network-based models like RNNs. | |