| No. | Question | Ans |
|---|---|---|
| 1 | **Deep-Reinforcement Learning >> Reinforcement Learning** QNo. 1: How can Reinforcement Learning learn optimal strategies to play against any opponent? A. By memorizing fixed opponent moves and responding with pre-programmed counter-actions based on historical game data. B. By exploring actions, receiving feedback through rewards, and adapting policies to maximize long-term gains against varying opponents. C. By using supervised datasets of expert games to predict opponent moves and optimize strategies accordingly. D. By randomly selecting moves without evaluation, relying on chance to discover winning strategies over repeated games. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): RL learns by trial and error, receiving rewards for good actions and penalties for bad ones, enabling it to adapt and optimize strategies regardless of opponent behavior. $\times$ Explanation of Distractors: A: Incorrect — Memorization limits adaptability; RL generalizes by learning from interaction, not fixed moves. B: Incorrect — This describes supervised learning, not RL's exploration and reward-based adaptation process. D: Incorrect — Random moves lack learning or optimization; RL requires feedback for weight updates. | |
| 2 | **Deep-Reinforcement Learning >> Reinforcement Learning** QNo. 2: What is the size of the search space in the Tic-Tac-Toe game? A. The total number of possible board configurations is $3^9$, or 19,683, considering each cell can be X, O, or empty. B. The total number of valid winning moves is 256, assuming only legal endgame scenarios and configurations. C. The search space size is infinite since players can repeat moves indefinitely without reaching a final outcome or terminal state. D. The game has 1000 unique states when played optimally, excluding invalid states and counting only symmetrical configurations once. | A |
| | **Explanantion:** $\checkmark$ Correct Answer: A $\checkmark$ Explanation of Correct Answer (A): Tic-Tac-Toe has 9 positions, each of which can be X, O, or empty. This leads to $3^9 = 19,683$ possible board configurations (though many are invalid or unreachable). $\times$ Explanation of Distractors: B: Incorrect — While 256 may be a guess for winning combinations, it doesn't reflect the full search space of all possible board states. C: Incorrect — The search space is finite, as the game board has only 9 cells with limited legal moves. D: Incorrect — The number 1000 is inaccurate and misleading; even accounting for symmetry, the unique board states are more numerous. | |
| 3 | **Deep-Reinforcement Learning >> Reinforcement Learning** QNo. 3: How is Reinforcement Learning (RL) different from Supervised Learning? A. RL learns through interaction with an environment using reward signals, while supervised learning uses labeled data with known outputs to guide training. B. RL requires labeled examples for each decision, whereas supervised learning explores state transitions by trial and error to learn optimal behavior. C. RL applies predefined logic to solve problems, while supervised learning trains models by randomly selecting outputs to maximize uncertainty in prediction. D. RL clusters similar behaviors using fixed rules, while supervised learning relies on feedback loops to optimize hidden layers automatically. | A |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (A): Reinforcement Learning learns by performing actions in an environment and receiving rewards or penalties, unlike supervised learning, which is trained directly on correct input-output pairs (labeled data). $\times$ Explanation of Distractors: B: Incorrect — This reverses the definitions; supervised learning uses labels, while RL learns from interaction and feedback. C: Incorrect — Neither learning type relies on predefined logic or random output selection; this is inaccurate. D: Incorrect — This describes neither RL nor supervised learning accurately; both rely on optimization, not rule-based clustering. | |
| 4 | **Deep-Reinforcement Learning >> Reinforcement Learning** QNo. 4: Why is Supervised Learning Predominant in machine learning applications today? A. It offers high accuracy to train models for real-world prediction and classification tasks by learning directly from labeled data, making it easier. B. It operates without the need for labeled data, reducing human involvement and improving scalability for tasks involving massive unstructured datasets. C. It builds internal representations using reward signals from the environment, making it suitable for dynamic decision-making in real-time settings. D. It clusters data based on hidden structures without any prior labels, offering flexibility in discovering natural groupings within unknown datasets. | A |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (A): Supervised learning dominates because it achieves high performance on classification and regression tasks by training on labeled datasets, where correct outputs are known to guide learning. $\times$ Explanation of Distractors: B: Incorrect — This describes unsupervised learning, which works without labels but typically yields lower accuracy for prediction tasks. C: Incorrect — This defines reinforcement learning, which focuses on learning through interaction, not labeled datasets. D: Incorrect — Again, this refers to unsupervised learning like clustering, which is more exploratory than predictive. | |
| 5 | **Deep-Reinforcement Learning >> Reinforcement Learning** QNo. 5: What are Heuristic Search Algorithms and their key characteristics? A. Algorithms that explore every possible path in the state space until the optimal solution is found through exhaustive brute-force search techniques. B. Algorithms that use fixed rules and logic tables to derive conclusions from given data without considering alternative or estimated paths. C. Algorithms that randomly explore the solution space without guidance, relying purely on probability to reach near-optimal solutions over time. D. Algorithms that use domain-specific knowledge or estimates to guide search toward goal states more efficiently than uninformed methods. | D |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (D): Heuristic search algorithms use problem-specific knowledge (heuristics) to estimate the best path toward a goal, improving efficiency over blind or uninformed search methods like BFS or DFS. $\times$ Explanation of Distractors: A: Incorrect — This describes uninformed brute-force search, not heuristic-guided search. B: Incorrect — This relates more to rule-based systems, not adaptive or goal-directed search algorithms. C: Incorrect — This better describes randomized or evolutionary algorithms, not informed search methods using heuristics. | |