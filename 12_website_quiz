| No. | Question | Ans |
|---|---|---|
| 1 | **Deep-Reinforcement Learning >> Monte Carlo & Temporal Difference** QNo. 1: When should Monte Carlo methods be preferred over Temporal Difference methods in real-world tasks? A. When online learning with immediate updates is necessary in fully observable environments. B. When models of the environment are available for fast planning and simulation-based learning. C. When only episodic data is available and tasks require learning from complete sequences of outcomes. D. When tasks require frequent updates using partial trajectories in a continuous, non-episodic simulation model. | D |
| | **Explanantion:** $\checkmark$ Correct Answer: C $\checkmark$ Explanation of Correct Answer (C): Correct. Monte Carlo methods are ideal when only complete episodic data is available, as they require full episode returns to update value estimates—perfect for batch, offline learning. $\times$ Explanation of Distractors: A: Incorrect. This suits Temporal Difference (TD) methods, which support online learning and update value estimates immediately after each step without needing full episode returns. B: Incorrect. This aligns with model-based reinforcement learning, not Monte Carlo, as methods don't require or use models for planning or simulation. D: Incorrect. TD methods are better for continuous or ongoing tasks with partial trajectories as they update estimates step-by-step without waiting for episodes to finish. | |
| 2 | **Deep-Reinforcement Learning >> Monte Carlo & Temporal Difference** QNo. 2: What is the update rule? TD: Temporal Difference MC: Monte Carlo A. TD(0) updates after episodes, using actual returns like Monte Carlo, for better convergence speed. B. TD(0) relies on future state sampling, while Monte Carlo uses only immediate rewards. C. TD(0) averages over full episodes, while Monte Carlo uses bootstrapped one-step estimates. D. TD(0) updates using immediate reward plus estimated value of the next state, while MC uses full return after episode. | D |
| | **Explanantion:** $\checkmark$ Correct Answer: D $\checkmark$ Explanation of Correct Answer (D): Correct. TD(0) uses the current reward plus the estimated value of the next state (bootstrapping). In contrast, Monte Carlo uses the full observed return after the episode ends. $\times$ Explanation of Distractors: A: Incorrect. TD(0) updates during the episode using bootstrapped estimates, not after episodes, unlike Monte Carlo. B: Incorrect. TD(0) doesn't sample future states—it estimates the value of the next state, while Monte Carlo uses the total return, not just immediate rewards. C: Incorrect. This reverses the definitions. Monte Carlo averages over full episode returns; TD(0) uses bootstrapped one-step updates during the episode. | |
| 3 | **Deep-Reinforcement Learning >> Monte Carlo & Temporal Difference** QNo. 3: Why is bootstrapping important in Temporal Difference (TD) methods? A. It allows for longer exploration of episodes before applying any updates to value functions. B. It avoids the need for using rewards and focuses on transitions only at the end of the episode. C. It enables updating values only after an entire episode has been fully completed and observed. D. It helps update estimates using current reward and predicted value without waiting for full episodes. | D |
| | **Explanantion:** $\checkmark$ Correct Answer: D $\checkmark$ Explanation of Correct Answer (D): Bootstrapping in TD methods lets value estimates be updated using the current reward plus the estimated value of the next state, enabling learning without waiting for episode completion. $\times$ Explanation of Distractors: A: Incorrect. TD methods update during episodes, not after long explorations. Bootstrapping allows quicker updates rather than delaying until the end of an episode. B: Incorrect. Bootstrapping still uses rewards—specifically the immediate reward and the estimated future values. It doesn't skip rewards or delay updates to the end. C: Incorrect. This describes Monte Carlo methods, which require full episodes; TD learning with bootstrapping updates estimates incrementally during episodes. | |
| 4 | **Deep-Reinforcement Learning >> Monte Carlo & Temporal Difference** QNo. 4: What is Temporal Difference learning? A. Learning by averaging full episode returns without updating during the episode. B. Learning by updating estimates using current reward plus estimated future value at each step. C. Learning by optimizing a policy using labeled supervised data from external sources. D. Learning through evolving populations of agents using random mutations and selection. | B |
| | **Explanantion:** $\checkmark$ Correct Answer: B $\checkmark$ Explanation of Correct Answer (B): TD learning updates value estimates incrementally using the reward from the current step and the estimated value of the next state (bootstrapping), allowing faster, online learning. $\times$ Explanation of Distractors: A: This describes Monte Carlo methods, which wait for an episode to finish before updating, in contrast to TD learning which updates estimates after each step using bootstrapping—i.e., current value estimates—not full episode returns. C: This defines supervised learning, where models are trained on labeled data. Temporal Difference learning doesn't use labeled datasets but learns through environment interaction. D: Describes evolutionary algorithms, where learning occurs via selection and mutation in populations. TD learning is based on value prediction, not genetic policy evolution. | |
| 5 | **Deep-Reinforcement Learning >> Monte Carlo & Temporal Difference** QNo. 5: What are Monte Carlo methods in reinforcement learning? A. Methods that estimate value functions by averaging returns from complete episodes without using bootstrapping or prior estimates. B. Algorithms that update values after every single step by relying on current estimates rather than final episode outcomes. C. Techniques that use random initialization and deep networks to simulate environment transitions without following any policy. D. A family of optimization approaches that use genetic selection to improve performance through randomized policy evolution. | A |
| | **Explanantion:** $\checkmark$ Correct Answer: A $\checkmark$ Correct. Monte Carlo (MC) methods estimate the value of a state or state-action pair by averaging actual returns from complete episodes. They do not bootstrap—meaning they don't rely on other estimates. $\times$ Explanation of Distractors: B: This describes Temporal-Difference (TD) learning, not Monte Carlo. TD updates values after every single step by relying on current estimates (bootstrapping), not final episode outcomes. C: This doesn't describe Monte Carlo methods, but vaguely resembles model-based RL or some unsupervised simulation methods, but MC methods always involve following a policy and observing actual returns. D: This describes evolutionary algorithms or genetic algorithms, not Monte Carlo methods. | |