| No. | Question | Ans |
|---|---|---|
| 1 | **Deep-Reinforcement Learning >> Dynamic Programming** QNo. 1: What are common pitfalls in dynamic programming implementation? A. Incorrect base cases or improper state transitions that cause wrong results or infinite recursion in recursive or tabular DP approaches. B. Overuse of pre-trained models that replace the need for defining recursive relationships in tabular data problems. C. Forgetting to shuffle data before applying dynamic programming to randomize exploration during policy evaluation or control. D. Using batch normalization without defining recurrent relations in transformer models during multi-stage pipeline training. | A |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (A): Dynamic programming relies on correctly defined base cases and state transitions that cause wrong results or infinite recursion. Carefully managing these components is critical for correctness and performance. $\times$ Explanation of Distractors: B: Incorrect — Pre-trained models are unrelated to classical dynamic programming or tabular DP issues. C: Incorrect — Shuffling data applies to training models, not to structured DP solutions with deterministic subproblem structures. D: Incorrect — Batch normalization and transformers are deep learning techniques, not related to common DP implementation issues. | |
| 2 | **Deep-Reinforcement Learning >> Dynamic Programming** QNo. 2: How does dynamic programming apply to speech recognition or part-of-speech tagging? A. By training deep neural networks using unsupervised methods to recognize language features and construct tagging rules manually. B. By using algorithms like Viterbi to efficiently find the most probable sequence of hidden states in structured prediction problems. C. By randomly assigning parts of speech and refining tags based on clustering of phonetically similar words across the input text. D. By generating all possible tag sequences and selecting the shortest one through exhaustive search and manual alignment. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): Dynamic programming is applied through the Viterbi algorithm, which is commonly used with Hidden Markov Models (HMMs) to efficiently find the most probable sequence of hidden states (like phonemes or POS tags) in sequence prediction tasks like speech recognition and part-of-speech tagging. $\times$ Explanation of Distractors: A: Incorrect — While neural networks are used in speech tasks, this does not explain the specific role of DP. C: Incorrect — POS tagging is not done through random assignment or clustering; DP ensures globally optimal paths, not approximations. D: Incorrect — Exhaustive search is computationally infeasible; DP avoids this by using the optimal substructure and overlapping subproblems. | |
| 3 | **Deep-Reinforcement Learning >> Dynamic Programming** QNo. 3: How is dynamic programming used in reinforcement learning algorithms? A. By generating random actions and rewards, then updating the agent's strategy using unsupervised clustering of previous experience logs. B. By iteratively evaluating and improving value functions or policies using the Bellman equation in a model-based environment. C. By memorizing all past state transitions and selecting the most frequent path for future decision-making in new environments. D. By using supervised labels to directly train the agent with correct action choices for every possible environment state. | B |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (B): Dynamic programming in reinforcement learning is used in model-based settings to compute or improve value functions or policies through iterative algorithms like value iteration or policy iteration, often based on the Bellman equation. $\times$ Explanation of Distractors: A: Incorrect — Random actions and unsupervised clustering are not characteristic of DP or standard reinforcement learning methods. C: Incorrect — RL doesn't rely on frequency counting alone; it optimizes long-term expected rewards through structured evaluation. D: Incorrect — Supervised learning uses labeled data; dynamic programming in RL focuses on learning from interaction with an environment model, not predefined labels. | |
| 4 | **Deep-Reinforcement Learning >> Dynamic Programming** QNo. 4: What is the difference between bottom-up (tabulation) and top-down (memoization) approaches in Dynamic Programming (DP)? A. Top-down directly iterates over all possible states, while bottom-up uses recursion with no memory to reduce time complexity. B. Bottom-up solves problems by storing only final answers, while top-down builds an answer list through repeated trial-and-error. C. Top-down solves problems recursively and stores results as needed, while bottom-up builds solutions iteratively starting from base cases. D. Bottom-up randomly samples subproblems in reverse, whereas top-down builds up from the base cases, processing each solution without storing previous answers. | C |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (C): Top-down (memoization) solves problems recursively and stores results as they are computed, while bottom-up (tabulation) builds the solution iteratively from the base case upward, avoiding recursion entirely. $\times$ Explanation of Distractors: A: Incorrect — Top-down is based on recursion, not iteration; bottom-up avoids recursion by iterating. B: Incorrect — Both approaches store results, but neither uses trial-and-error or stores only final answers. D: Incorrect — There is no random sampling in bottom-up, and top-down always stores intermediate results if implemented correctly. | |
| 5 | **Deep-Reinforcement Learning >> Dynamic Programming** QNo. 5: What is Dynamic Programming? A. A method that randomly selects solutions and stores only the final one to speed up repeated computations in large search problems. B. A way of optimizing algorithms by parallelizing recursive calls without considering overlapping subproblems or storing previous results. C. A brute-force approach to enumerate all possible solutions by exhaustively searching the entire space of potential outcomes. D. A technique that solves complex problems by breaking them into overlapping subproblems and storing solutions to avoid redundant computation. | D |
| | **Explanantion:** $\checkmark$ Explanation of Correct Answer (D): Dynamic Programming (DP) is an optimization technique that breaks a problem into smaller, overlapping subproblems. It stores intermediate results in memory to avoid recomputing them, making it highly efficient for recursive problems. $\times$ Explanation of Distractors: A: Incorrect — Random selection does not apply in DP; it's a deterministic method that relies on storing structured subproblem solutions. B: Incorrect — DP involves memoization/tabulation, not blind parallel recursion. It crucially relies on storing results. C: Incorrect — Brute-force is inefficient and unrelated to the key storage and reuse mechanism in DP. | |